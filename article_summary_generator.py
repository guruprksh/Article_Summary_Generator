# -*- coding: utf-8 -*-
"""Article Summary Generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BhdTGQxYxOYRkkxklgbhBo1TG8pFN3e8

#Article Summary Generator

*   Data collection through web scraping
*   Data Cleaning
*   NLTK to build Token
*   Word Frequency
*   weighted frequency for each words
*   Calculate score for each sentence
*   Select Paragraph for summary
"""

from urllib import request
from bs4 import BeautifulSoup as bs
import re
import nltk
import heapq

nltk.download('stopwords')
nltk.download('punkt')

url = input("Enter article source (url): ")
allparaContent = ""

#crawl as html
htmlDoc = request.urlopen(url)

soupObject = bs(htmlDoc, 'html.parser')
paraContents = soupObject.findAll('p')
#print(paraContents)

for paraContent in paraContents:
    allparaContent += paraContent.text
print(allparaContent)

"""##Data Cleaning"""

allparaContent_cleanedData = re.sub(r'\[[0-9]*\]',' ', allparaContent)
allparaContent_cleanedData = re.sub(r'\s+',' ', allparaContent_cleanedData)
print(allparaContent_cleanedData)

"""##Sentence Token Creation"""

sentence_tokens = nltk.sent_tokenize(allparaContent_cleanedData)
words_tokens = nltk.word_tokenize(allparaContent_cleanedData)

"""##Word Frequency Calculation"""

stopwords = nltk.corpus.stopwords.words('english')
word_frequencies = {}

for word in words_tokens:
    if word not in stopwords:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        else:
            word_frequencies[word] +=1

print(word_frequencies)

"""##Weighted Word Frequency Calculation"""

max_word_frequencies = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/max_word_frequencies)
print(word_frequencies)

#sentence score with each word weighted frequency

sentence_scores = {}

for sentence in sentence_tokens:
    for word in nltk.word_tokenize(sentence.lower()):
        if word in word_frequencies.keys():
            if (len(sentence.split(' ')))<30:
                if sentence not in sentence_scores.keys():
                    sentence_scores[sentence] = word_frequencies[word]
                else:
                    sentence_scores[sentence] +=word_frequencies[word]

print(sentence_scores)

"""#Article Summary"""

summary_Article = heapq.nlargest(10, sentence_scores, key = sentence_scores.get)
print(summary_Article)